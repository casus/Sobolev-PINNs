{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, './PSM_V1')\n",
    "from sobolev import Sobolev\n",
    "from solver import Solver\n",
    "from utils import matmul\n",
    "from diffeomorphisms import hyper_rect\n",
    "import surrogates\n",
    "import matplotlib.pyplot as plt\n",
    "from pinnutils import PINN\n",
    "#from pinnutils import PINNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "# a=-1.0, b=1.0, q=3, sob_2d(deg=30), sob_1d(deg=100), model(n=30, p=np.inf), s=[0,-1], optimizer:LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Poisson Equation\n",
    "Let $\\Omega = (-1 , 1)^2, A, C, \\omega, \\beta \\in \\mathbb{R}$:\n",
    "$${\\large\\begin{cases}\n",
    "        \\Delta u + f = 0, (x,y) \\in \\Omega\\\\\n",
    "        u = g, (x,y) \\in \\partial \\Omega\n",
    "    \\end{cases}}$$\n",
    "with:\n",
    "$$\n",
    "    {\\large f(x,y) = 2\\lambda^2 cos(\\lambda x) sin(\\lambda y)}\\\\\n",
    "    {\\large g(x,y) = cos(\\lambda x) sin(\\lambda y)}\\\\\n",
    "    {\\large \\lambda = 2 \\pi q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = np.array([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "\n",
    "q = 1/2\n",
    "lam = 2*np.pi*q\n",
    "\n",
    "def f(x,y):\n",
    "    return 2*lam**2*np.cos(lam*x)*np.sin(lam*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobolev Cubature\n",
    "diffeo_2d = hyper_rect(*rect)\n",
    "diffeo_1d_0 = hyper_rect(rect[0])\n",
    "diffeo_1d_1 = hyper_rect(rect[1])\n",
    "\n",
    "sob_2d = Sobolev(deg=30, dim=2, diffeo=diffeo_2d)\n",
    "sob_1d_0 = Sobolev(deg=100, diffeo=diffeo_1d_0)\n",
    "sob_1d_1 = Sobolev(deg=100, diffeo=diffeo_1d_1)\n",
    "\n",
    "dx2, dy2 = torch.tensor(sob_2d.diff.diffs(np.array([[2,0],[0,2]])), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7383, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Beta(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, param_size =1):\n",
    "        super(Beta, self).__init__()\n",
    "        self.param_size = param_size\n",
    "        self.fc_In = torch.nn.Linear(1,1, bias =  False)\n",
    "\n",
    "    def set_weights(self, val):\n",
    "        with torch.no_grad():\n",
    "            self.fc_In.weight.copy_(val*torch.ones_like(self.fc_In.weight))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_In.weight*torch.ones_like(x)\n",
    "        return out\n",
    "# Surrogate Model for the Inference parameter\n",
    "lam_t = Beta()\n",
    "lam_t(torch.tensor([0]))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt(x,y):\n",
    "    return np.cos(lam*x)*np.sin(lam*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |--------------------------------------------|\n",
    "# |  Operator  |          Formulation          |  \n",
    "# |------------|-------------------------------|\n",
    "# | id         |  L2 grad of L2                | \n",
    "# | m_inv      |  L2 grad of Sob               |\n",
    "# | weak m_inv |  L2 grad of weak Sob          |\n",
    "# | m          |  L2 grad of negative Sob      |\n",
    "# | weak m     |  L2 grad of weak negative Sob |\n",
    "# |--------------------------------------------|\n",
    "#\n",
    "# For that use:\n",
    "# -> sob.set_s(s)\n",
    "# -> sob.metric(rev=False/True, weak=False/True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sobolev Order\n",
    "sob_2d.set_s(0)\n",
    "sob_1d_0.set_s(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132014/1726985390.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  xs = torch.tensor(xs)\n",
      "/trinity/shared/pkg/devel/python/3.10.4/venv/venv_a/lib/python3.10/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/tmp/ipykernel_132014/1726985390.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_pde = torch.tensor(torch.cat((X_t.reshape(-1,1), Y_t.reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
      "/tmp/ipykernel_132014/1726985390.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_bdl = torch.tensor(torch.cat((xs_bdx.reshape(-1,1), rect[1][0]*torch.ones(len(xs_bdx)).reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
      "/tmp/ipykernel_132014/1726985390.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_bdr = torch.tensor(torch.cat((xs_bdx.reshape(-1,1), rect[1][1]*torch.ones(len(xs_bdx)).reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
      "/tmp/ipykernel_132014/1726985390.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_bdl = torch.tensor(torch.cat((rect[0][0]*torch.ones(len(xs_bdx)).reshape(-1,1),xs_bdx.reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
      "/tmp/ipykernel_132014/1726985390.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_bdr = torch.tensor(torch.cat((rect[0][1]*torch.ones(len(xs_bdx)).reshape(-1,1),xs_bdx.reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
      "/tmp/ipykernel_132014/1726985390.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_2d = torch.tensor(sob_2d.get_leja_weights(), dtype = torch.float32)\n",
      "/tmp/ipykernel_132014/1726985390.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_1d = torch.tensor(sob_1d_0.get_leja_weights(), dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "_, xs_bndr_0 = sob_1d_0.get_xs()\n",
    "_, xs_bndr_1 = sob_1d_1.get_xs()\n",
    "xs_plt, xs = sob_2d.get_xs()\n",
    "leja_grid_2d = sob_2d.get_leja_grid()\n",
    "xs = torch.tensor(xs)\n",
    "xs_bdx =  torch.tensor(xs_bndr_0[0])\n",
    "X_t, Y_t = torch.meshgrid(xs[0],xs[1])\n",
    "X_pde = torch.tensor(torch.cat((X_t.reshape(-1,1), Y_t.reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
    "X_bdl = torch.tensor(torch.cat((xs_bdx.reshape(-1,1), rect[1][0]*torch.ones(len(xs_bdx)).reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
    "X_bdr = torch.tensor(torch.cat((xs_bdx.reshape(-1,1), rect[1][1]*torch.ones(len(xs_bdx)).reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
    "Y_bdl = torch.tensor(torch.cat((rect[0][0]*torch.ones(len(xs_bdx)).reshape(-1,1),xs_bdx.reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
    "Y_bdr = torch.tensor(torch.cat((rect[0][1]*torch.ones(len(xs_bdx)).reshape(-1,1),xs_bdx.reshape(-1,1)),1), dtype = torch.float32, requires_grad = True)\n",
    "u_bdxl = torch.tensor(gt(xs_bndr_0[0], rect[1][0].reshape(-1)), dtype = torch.float32)\n",
    "u_bdxr = torch.tensor(gt(xs_bndr_0[0], rect[1][1].reshape(-1)), dtype = torch.float32)\n",
    "u_bdyl= torch.tensor(gt(rect[0][0].reshape(-1,1), xs_bndr_1[0]).reshape(-1), dtype = torch.float32)\n",
    "u_bdyr= torch.tensor(gt(rect[0][1].reshape(-1,1), xs_bndr_1[0]).reshape(-1), dtype = torch.float32)\n",
    "fXY = f(X_t,Y_t).reshape(-1)\n",
    "gXY = gt(X_t,Y_t).reshape(-1)\n",
    "w_2d = torch.tensor(sob_2d.get_leja_weights(), dtype = torch.float32)\n",
    "w_1d = torch.tensor(sob_1d_0.get_leja_weights(), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sobolev Metrics\n",
    "metric_2d = sob_2d.metric(weak=True)\n",
    "metric_1d_0 = sob_1d_0.l2_metric()\n",
    "metric_1d_1 = sob_1d_1.l2_metric()\n",
    "\n",
    "# Formulation\n",
    "K = dx2+dy2\n",
    "eq = lambda u: matmul(K, u)+fXY\n",
    "crit_pde = lambda u: sob_2d.loss(eq(u), weak=True)\n",
    "crit_bdxl = lambda u: sob_1d_0.l2_loss(u-u_bdxl)\n",
    "crit_bdxr = lambda u: sob_1d_0.l2_loss(u-u_bdxr)\n",
    "crit_bdyl = lambda u: sob_1d_0.l2_loss(u-u_bdyl)\n",
    "crit_bdyr = lambda u: sob_1d_0.l2_loss(u-u_bdyr)\n",
    "\n",
    "crit_rec = lambda u: torch.sum(((u-gt_)*w_2d)**2)\n",
    "eq = lambda u, lam: matmul(K, u)+lam*gXY\n",
    "crit_dmn = lambda u, lam: sob_2d.loss(eq(u, lam), weak=True)\n",
    "#grad_dmn = lambda u: 2*matmul(K.T, metric_2d(eq(u)))\n",
    "#grad_bndr_0 = lambda u: 2*metric_1d_0(u-u_bndr_0)\n",
    "#grad_bndr_1 = lambda u: 2*metric_1d_1(u-u_bndr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD = np.concatenate([X_pde.detach().numpy(),X_bdl.detach().numpy(), X_bdr.detach().numpy(), Y_bdl.detach().numpy() , Y_bdl.detach().numpy()], 0)\n",
    "# compute mean and std of training data\n",
    "X_mean = torch.tensor(np.mean(TD, axis=0, keepdims=True),device=device)\n",
    "X_std = torch.tensor(np.std(TD, axis=0, keepdims=True),device=device)\n",
    "seedc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import grad\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from itertools import product, combinations\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, MultiStepLR\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "class sine(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return torch.sin(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure(optim):\n",
    "        optim.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_s = torch.max(torch.linalg.eig(dx2)[0].real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#parameters: 7851\n",
      "epoch 25881/30000, loss=0.0000000253, lambda=1.0000, lr=0.00001\t\t\t\r"
     ]
    }
   ],
   "source": [
    "net = PINN(sizes=[2,50,50,50,50,1], mean=X_mean, std=X_std, seed=seedc, activation=sine()).to(device)\n",
    "print(\"#parameters:\", sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "n_epochs   = 30000\n",
    "lamb  = 1\n",
    "losses_bc  = [];\n",
    "losses_reg = [];\n",
    "params = [{'params': net.parameters()}]\n",
    "milestones = [[15000,25000]]\n",
    "optimizer = torch.optim.Adam(params)\n",
    "optim_lam_t =  torch.optim.Adam(lam_t.parameters())\n",
    "scheduler = MultiStepLR(optimizer, milestones[0], gamma=0.1)\n",
    "start_time = time.time()\n",
    "ds = 1\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    uhat  = net(X_pde).T[0]\n",
    "    #l_pde   = crit_pde(uhat)\n",
    "    optim_lam_t.zero_grad()\n",
    "    lam_p = lam_t(torch.tensor([0]))[0][0]\n",
    "    \n",
    "    l_pde = torch.sum(((eq(uhat,2*lam_p**2)*w_2d)**2))\n",
    "    pred_rec = net(X_pde).T[0]\n",
    "    \n",
    "    l_rec = torch.sum(((pred_rec-gXY)*w_2d)**2) \n",
    "    \n",
    "    loss = l_pde/(lam_s)+ l_rec\n",
    "    losses_reg.append(l_pde.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optim_lam_t.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"epoch {}/{}, loss={:.10f}, lambda={:.4f}, lr={:,.5f}\\t\\t\\t\"\n",
    "          .format(epoch+1, n_epochs, loss.item(), lamb, optimizer.param_groups[0]['lr']), end=\"\\r\")\n",
    "        \n",
    "elapsed_time = time.time() - start_time\n",
    "print('CPU time = ',elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = torch.meshgrid(torch.linspace(-1,1,100),torch.linspace(-1,1,100))\n",
    "X_T = torch.cat((X_test.reshape(-1,1), Y_test.reshape(-1,1)),1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 25), dpi=80)\n",
    "u_sol = gt(X_test,Y_test).detach().numpy()\n",
    "out = net(X_T).reshape(100,100).detach().numpy()\n",
    "#out = N_p()._eval(X_r).reshape(100,100)\n",
    "L0_inf = np.max(abs(out.reshape(-1)-u_sol.reshape(-1)))\n",
    "#Lp_inf = torch.max(abs(poisson_residual(net_s(inp_r),inp_r,omega).reshape(-1)))\n",
    "L0_mean =np.mean(abs(out.reshape(-1)-u_sol.reshape(-1)))\n",
    "print(\"pred rel. linf-error = {:e}\".format(L0_inf))\n",
    "print(\"pred rel. l2-error = {:e}\".format(L0_mean))\n",
    "#print(\"pde res. linf-error = {:e}\".format(Lp_inf))\n",
    "print(\"pred_rel_std. linf-error = {:e}\".format(np.std(abs(out.reshape(-1)-u_sol.reshape(-1)))))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(u_sol, cmap=\"Spectral\", origin=\"lower\")\n",
    "#plt.xticks(np.arange(0,len(x)+1,25), np.arange(0, 1.1, 0.25))\n",
    "#plt.yticks(np.arange(0,len(y)+1,25), np.arange(0, 1.1, 0.25))\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.clim(-1,1)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(out, cmap=\"Spectral\", origin=\"lower\")\n",
    "#plt.xticks(np.linspace(0,len(x)+1,5),np.linspace(-1, 1, 5))\n",
    "#plt.yticks(np.linspace(0,len(x)+1,5),np.linspace(-1, 1, 5))\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.title(\"Prediction\")\n",
    "plt.clim(-1,1)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "#plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.abs(out-u_sol), cmap=\"Spectral\", origin=\"lower\")\n",
    "#plt.xticks(np.linspace(0,len(x)+1,5),np.linspace(-1, 1, 5))\n",
    "#plt.yticks(np.linspace(0,len(x)+1,5),np.linspace(-1, 1, 5))\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.title(\"Point-wise Error\")\n",
    "plt.clim(0,0.2)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "#plt.gcf().set_size_inches(14,4)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(folder + 'pred_error_MSE.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(lam_p)-lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
